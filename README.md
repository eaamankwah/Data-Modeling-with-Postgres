# Data-Modeling-with-Postgres

# Table of Contents
* Overview
* Dataset
* Star Database Schema
* * Fact Table
* * Dimension Tables
* ETL Pipeline using Python
* * Running ETL Process
* Project Files 
* References

## Overview
This project is first of the Udacity Data Engineering Nanodegree. 

The objective of the project is to create a Postgres database (sparkifydb) with tables designed to optimize queries on song play analysis. A database star schema consisting of one fact table and four dimension tables and ETL pipeline for this analysis will be created using Python and SQL. The ETL pipeline will be used to transfer data from files in two local directories into the tables in Postgres.

## Dataset

The dataset is a collected by and online startup platform called Sparkify who wants to analyze the data they've been collecting on songs and user activity on their new music streaming app. Sparkify wants to access the songs  that their users are listening to. The  data currently resides in two directories namely,  a directory of JSON logs on user activity on the app and  a directory with JSON metadata on the songs in their app.

**Song Dataset**
The first dataset is a subset of real data from the [Million Song Dataset](http://millionsongdataset.com/). Each file is in JSON format and contains metadata about a song and the artist of that song. The files are partitioned by the first three letters of each song's track ID. 

For example,  below are file paths to two files in this dataset.
song_data/A/B/C/TRABCEI128F424C983.json 
song_data/A/A/B/TRAABJL12903CDCF1A.json 

**Log Dataset**
The second dataset consists of log files in JSON format generated by an [event simulator](https://github.com/Interana/eventsim) based on the songs in the dataset above. These simulate activity logs from a music streaming app based on specified configurations.

The log files in the dataset are partitioned by year and month. For example, here are file paths to two files in this dataset.

log_data/2018/11/2018-11-12-events.json 
log_data/2018/11/2018-11-13-events.json

## Star Database Schema

The star schema was used for this project and has the following benefits:

* Queries are simpler: Because all of the data connects through the fact table the multiple dimension tables are treated as one large table of information, and that makes queries simpler and easier to perform.
* Easier business insights reporting: Star schemas simplify the process of pulling business reports like "what songs users are listening to".
* Better-performing queries: By removing the bottlenecks of a highly normalized schema, query speed increases, and the performance of read-only commands improves.
* Provides data to OLAP systems: OLAP (Online Analytical Processing) systems can use star schemas to build OLAP cubes.
* Since the data set is small and structured, the star schema is sufficient to model using ERD models and can easily be queried using SQL joins

The image below indicates the architectural diagram the database star schema of the the project:

![schema](https://github.com/eaamankwah/Data-Modeling-with-Postgres/blob/main/screenshots/Song_ERD.png)

### Fact Table
The main fact table which contains all the measures associted with each event (user song plays) is shown below:

#### Songplays Table

| COLUMN      | TYPE      | CONSTRAINT      |
|---    |---    |---    |    
|   songplay_id    | SERIAL      |   PRIMARY KEY    | 
|   start_time    |  TIMESTAMP    |      | 
|   user_id    |   int    |   NOT NULL    | 
|   level    |   text |       | 
|   song_id    |   text    |       | 
|   artist_id    |   text    |       | 
|   session_id    |   int    |       | 
|   location    |   text    |       | 
|   user_agent    |   text    |       | 

The songplay_id field has the primary key constraint and it is an auto-incremental value.

The query that inserts records into this table is:

songplay_table_insert = ("""
'' INSERT INTO songplays
(start_time, user_id, level, song_id, artist_id, session_id, location, user_agent)
VALUES (%s, %s, %s, %s, %s, %s, %s, %s)
ON CONFLICT (songplay_id) DO NOTHING; ''

### Dimension Table

The dimension tables below contain detailed information about each row in the fact table.

#### User Table

| COLUMN      | TYPE      | CONSTRAINT      |
|---    |---    |---    |    
|   user_id    | int      |   PRIMARY KEY    | 
|   first_name    |   test    |    NOT NULL  | 
|   last_name    |   text    |   NOT NULL   | 
|   gender    |   text |       | 
|   level    |   text    |       | 

The query that inserts records into this table is:

'' INSERT INTO users
(user_id, first_name, last_name, gender, level)
VALUES (%s, %s, %s, %s, %s)
ON CONFLICT (user_id) DO NOTHING; ''

#### Songs Table

| COLUMN      | TYPE      | CONSTRAINT       |
|---    |---    |---    |    
|   song_id    |    text     |   PRIMARY KEY    | 
|   title    |   text    |   NOT NULL   | 
|   artist_id    |   text    |   NOT NULL   | 
|   year    |   int |       | 
|   duration    |   float    |    NOT NULL   | 

The query that inserts records into this table is:

'' INSERT INTO songs
(song_id, title, artist_id, year, duration)
VALUES (%s, %s, %s, %s, %s)
ON CONFLICT (song_id) DO NOTHING; ''

#### Artists Table 

| COLUMN      | TYPE      | CONSTRAINT       |
|---    |---    |---    |    
|   artist_id    |    text     |   PRIMARY KEY    | 
|   name    |   text    |    NOT NULL   | 
|   location    |   text    |       | 
|   latitude    |   float    |       | 
|   longitude    |   float   |       | 

The query that inserts records into this table is:

'' INSERT INTO artists
(artist_id, name, location, lattitude, longitude)
VALUES (%s, %s, %s, %s, %s)
ON CONFLICT (artist_id) DO NOTHING; ''

#### Time Table 

| COLUMN      | TYPE      | CONSTRAINT       |
|---    |---    |---    |    
|   start_time    |  TIMESTAMP      |   PRIMARY KEY    | 
|   hour    |   int    |       | 
|   day    |   int    |       | 
|   week    |   int    |       | 
|   month    |   int    |       | 
|   year    |   int    |       | 
|   weekday    |   text    |       | 

The query that inserts records into this table is:

'' INSERT INTO time
(start_time, hour, day, week, month, year, weekday)
VALUES (%s, %s, %s, %s, %s, %s, %s)
ON CONFLICT (start_time) DO NOTHING; ''

## ETL Pipeline using Python

The ETL pipeline  mainly extract data from the log files and insert them into the fact and dimensiona tables.

### The Queries to the database

The sql_queries.py contains all the code that runs queries to the database. Its main function includes
* The CREATE codes that construct all the tables
* The INSERT codes that populate the tables with records and 
* The SELECT statements to get artist_id and song_id to populate the songplays table.

### Running ETL Process

1.  A connection was stablished to the database.
2. Song data and artists data were inserted into the songs and artists tables, respectively.
3. The Unix timestamp (ts) was inserted into the time table and the time components ( year, day, hour, week, month and dayofweek) were extracted from the ts field.
4. The user information was inserted in the users table.
5. The songplays records were inserted into songplays table. 
5. The database was then closed and disconnected

## Project Files

1. data - folder contains the data files, including log and song datasets.
2. test.ipynb  - displays the first few rows of each table to check the proper functioning of the database.
3. create_tables.py - drops and creates  tables in order to reset tables before each time the ETL scripts are run.
4. etl.ipynb - reads and processes a single file from song_data and log_data and loads the data into tables. This notebook contains detailed instructions on the ETL process for each of the tables.
5. etl.py - reads and processes files from song_data and log_data and loads them into tables. The file is created based on the ETL notebook.
6. sql_queries.py - contains all the queries, and is imported into the test.ipynb, creat_tables.py and etl.py files.
7. README.md - provides discussion on your project.

References

* [Xplenty](https://www.xplenty.com/integrations/)

* [Udacity Q & A Platform](https://knowledge.udacity.com/?nanodegree=nd027&page=1&project=572&rubric=2500)


